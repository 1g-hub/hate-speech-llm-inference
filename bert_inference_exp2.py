# -*- coding: utf-8 -*-
"""bert_inference_Exp2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1UjixI0Ern9TuK80TcDx0dZll5h1GgZNY
"""

!pip install datasets

from datasets import load_dataset

data = load_dataset("hatexplain")

# print train

print(data['train'])

# print annotators

print(data['train'].features['annotators'])

# print the annotators of few exemples

print(data['train'][0]['annotators'])
print(data['train'][1]['annotators'])
print(data['train'][2]['annotators'])

# print the post_tokens of few exemples

print(data['train'][0]['post_tokens'])
print(data['train'][1]['post_tokens'])
print(data['train'][2]['post_tokens'])

import pandas as pd

# print few example of the hatexplain training dataset in a table

train_df = data["train"]
train_df = train_df.to_pandas()
print(train_df.head())

print(data['train'][450]['post_tokens'])

from datasets import load_dataset

# Load the HateXplain dataset
dataset = load_dataset("hatexplain")

# Inspect the structure of the 'annotators' field
print(dataset['train']['annotators'][:5])

import pandas as pd

# Step 1: Load offensive words from profanity.txt
with open("profanity.txt") as f:
    offensive_words = set(word.strip().lower() for word in f.readlines())

# Step 2: Function to calculate offensiveness level
def calculate_offensive_score(text):
    """
    Calculates offensiveness score as the number of offensive words in the text.
    """
    words = text.lower().split()  # Split text into words (lowercase for case-insensitive matching)
    score = sum(1 for word in words if word in offensive_words)  # Count matches in the offensive words set
    return score

# Step 3: Example dataset
data = {
    "text": [
        "fuck",
        "This is absolutely abso-fucking-lutely nonsense.",
        "All cats are gray in the dark, nothing offensive here.",
        "AIDS is a sensitive topic.",
        "Dumb and dumber, the sequel."
    ]
}

# Step 4: Convert to DataFrame
df = pd.DataFrame(data)

# Step 5: Calculate offensiveness level for each sentence
df["offensiveness_level"] = df["text"].apply(calculate_offensive_score)

# Display the DataFrame
print(df)

from transformers import BertTokenizer, BertForSequenceClassification

# Load the model and tokenizer
model = BertForSequenceClassification.from_pretrained("saved_hate_model")
tokenizer = BertTokenizer.from_pretrained("saved_hate_model")

print("Model and tokenizer have been loaded from './saved_hate_model'.")

import torch

# Function to predict hatespeech/normal and calculate offensiveness
def predict_hatespeech_and_offensiveness(model, text, tokenizer):
    # Determine the device (use GPU if available, otherwise CPU)
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

    # Move the model to the correct device
    model.to(device)

    # Tokenize the text and move inputs to the correct device
    inputs = tokenizer(text, return_tensors="pt", truncation=True, padding=True, max_length=128)
    input_ids = inputs["input_ids"].to(device)
    attention_mask = inputs["attention_mask"].to(device)

    # Predict hatespeech/normal
    model.eval()  # Set model to evaluation mode
    with torch.no_grad():
        outputs = model(input_ids=input_ids, attention_mask=attention_mask)
        logits = outputs.logits
        predicted_label = torch.argmax(logits, dim=1).item()  # Binary classification

    # Calculate offensiveness level
    offensive_score = calculate_offensive_score(text)

    # Map predicted label to readable format
    label_text = "hatespeech" if predicted_label == 0 else "normal"

    return label_text, offensive_score

# Test the pipeline with a sample sentence
sample_sentence = "You are such a 4uck and an idiot!"
predicted_label, offensive_score = predict_hatespeech_and_offensiveness(model, sample_sentence, tokenizer)

print(f"Sentence: {sample_sentence}")
print(f"Predicted Label: {predicted_label}")
print(f"Offensiveness Score: {offensive_score}")

from datasets import load_dataset
from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments
import torch
from sklearn.model_selection import train_test_split
import pandas as pd
from collections import Counter

# Load the HateXplain dataset
dataset = load_dataset("hatexplain")

# Prepare the text and labels
texts = [' '.join(item) for item in dataset['train']['post_tokens']]
labels = [max(Counter(item['label']).items(), key=lambda x: x[1])[0] for item in dataset['train']['annotators']]

# Filter out offensive labels (label '2' assumed to be offensive)
filtered_texts = [text for text, label in zip(texts, labels) if label != 2]
filtered_labels = [label for label in labels if label != 2]

# Remap labels: 0 for hatespeech, 1 for normal
label_mapping = {0: 0, 1: 1}  # {hatespeech: 0, normal: 1}
mapped_labels = [label_mapping[label] for label in filtered_labels]

# Convert to DataFrame
df = pd.DataFrame({'text': filtered_texts, 'label': mapped_labels})

# Split the dataset and reset indices
train_texts, test_texts, train_labels, test_labels = train_test_split(df['text'], df['label'], test_size=0.2, random_state=42)
train_texts = train_texts.reset_index(drop=True)
test_texts = test_texts.reset_index(drop=True)
train_labels = train_labels.reset_index(drop=True)
test_labels = test_labels.reset_index(drop=True)

# Load the tokenizer
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')

# Tokenize the dataset
train_encodings = tokenizer(train_texts.tolist(), truncation=True, padding=True, max_length=128)
test_encodings = tokenizer(test_texts.tolist(), truncation=True, padding=True, max_length=128)

import random
import torch

# Function to predict hatespeech/normal and calculate offensiveness
def predict_hatespeech_and_offensiveness(model, text, tokenizer):
    # Determine the device (use GPU if available, otherwise CPU)
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

    # Move the model to the correct device
    model.to(device)

    # Tokenize the text and move inputs to the correct device
    inputs = tokenizer(text, return_tensors="pt", truncation=True, padding=True, max_length=128)
    input_ids = inputs["input_ids"].to(device)
    attention_mask = inputs["attention_mask"].to(device)

    # Predict hatespeech/normal
    model.eval()  # Set model to evaluation mode
    with torch.no_grad():
        outputs = model(input_ids=input_ids, attention_mask=attention_mask)
        logits = outputs.logits
        predicted_label = torch.argmax(logits, dim=1).item()  # Binary classification

    # Calculate offensiveness level
    offensive_score = calculate_offensive_score(text)

    # Map predicted label to readable format
    label_text = "hatespeech" if predicted_label == 0 else "normal"

    return label_text, offensive_score

# Function to test a random example from the test dataset
def test_random_example(model, tokenizer, test_texts):
    # Select a random example from the test_texts (list of strings)
    random_idx = random.randint(0, len(test_texts) - 1)
    random_text = test_texts[random_idx]

    # Predict hatespeech/normal and calculate offensiveness
    predicted_label, offensive_score = predict_hatespeech_and_offensiveness(model, random_text, tokenizer)

    # Print the results
    print(f"Random Sentence: {random_text}")
    print(f"Predicted Label: {predicted_label}")
    print(f"Offensiveness Score: {offensive_score}")

# Test the function with the model and test dataset
test_random_example(model, tokenizer, test_texts)

import random
import torch

# Function to predict hatespeech/normal and calculate offensiveness
def predict_hatespeech_and_offensiveness(model, text, tokenizer):
    # Determine the device (use GPU if available, otherwise CPU)
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

    # Move the model to the correct device
    model.to(device)

    # Tokenize the text and move inputs to the correct device
    inputs = tokenizer(text, return_tensors="pt", truncation=True, padding=True, max_length=128)
    input_ids = inputs["input_ids"].to(device)
    attention_mask = inputs["attention_mask"].to(device)

    # Predict hatespeech/normal
    model.eval()  # Set model to evaluation mode
    with torch.no_grad():
        outputs = model(input_ids=input_ids, attention_mask=attention_mask)
        logits = outputs.logits
        predicted_label = torch.argmax(logits, dim=1).item()  # Binary classification

    # Calculate offensiveness level
    offensive_score = calculate_offensive_score(text)

    # Map predicted label to readable format
    label_text = "hatespeech" if predicted_label == 0 else "normal"

    return label_text, offensive_score, predicted_label

# Function to test a random example from the test dataset
def test_random_example(model, tokenizer, test_texts, test_labels):
    # Select a random example from the test_texts (list of strings)
    random_idx = random.randint(0, len(test_texts) - 1)
    random_text = test_texts[random_idx]
    true_label = test_labels[random_idx]  # Get the true label for this example

    # Predict hatespeech/normal and calculate offensiveness
    predicted_label_text, offensive_score, predicted_label = predict_hatespeech_and_offensiveness(model, random_text, tokenizer)

    # Map true label to readable format
    true_label_text = "hatespeech" if true_label == 0 else "normal"

    # Print the results
    print(f"Random Sentence: {random_text}")
    print(f"True Label: {true_label_text}")
    print(f"Predicted Label: {predicted_label_text}")
    print(f"Offensiveness Score: {offensive_score}")
test_random_example(model, tokenizer, test_texts, test_labels)

# Function to find and test misclassified examples
def test_random_misclassified_example(model, tokenizer, test_texts, test_labels):
    misclassified = []

    # Find all misclassified examples
    for idx in range(len(test_texts)):
        text = test_texts[idx]
        true_label = test_labels[idx]

        # Predict hatespeech/normal
        predicted_label_text, _, predicted_label = predict_hatespeech_and_offensiveness(model, text, tokenizer)

        if predicted_label != true_label:
            misclassified.append((text, true_label, predicted_label))

    # Select a random misclassified example
    if misclassified:
        random_idx = random.randint(0, len(misclassified) - 1)
        random_text, true_label, predicted_label = misclassified[random_idx]

        # Map labels to readable format
        true_label_text = "hatespeech" if true_label == 0 else "normal"
        predicted_label_text = "hatespeech" if predicted_label == 0 else "normal"

        # Print the misclassified example
        print(f"Random Misclassified Sentence: {random_text}")
        print(f"True Label: {true_label_text}")
        print(f"Predicted Label: {predicted_label_text}")
    else:
        print("No misclassified examples found!")

# Test the function with the model and test dataset

test_random_misclassified_example(model, tokenizer, test_texts, test_labels)

import random
import torch

# Function to predict hatespeech/normal and calculate offensiveness
def predict_hatespeech_and_offensiveness(model, text, tokenizer):
    # Determine the device (use GPU if available, otherwise CPU)
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

    # Move the model to the correct device
    model.to(device)

    # Tokenize the text and move inputs to the correct device
    inputs = tokenizer(text, return_tensors="pt", truncation=True, padding=True, max_length=128)
    input_ids = inputs["input_ids"].to(device)
    attention_mask = inputs["attention_mask"].to(device)

    # Predict hatespeech/normal
    model.eval()  # Set model to evaluation mode
    with torch.no_grad():
        outputs = model(input_ids=input_ids, attention_mask=attention_mask)
        logits = outputs.logits
        predicted_label = torch.argmax(logits, dim=1).item()  # Binary classification

    # Calculate offensiveness level
    offensive_score = calculate_offensive_score(text)

    # Map predicted label to readable format
    label_text = "hatespeech" if predicted_label == 0 else "normal"

    return label_text, offensive_score, predicted_label

# Function to find and print misclassified examples
def print_misclassified_examples(model, tokenizer, test_texts, test_labels, num_examples=10):
    misclassified = []

    # Find all misclassified examples
    for idx in range(len(test_texts)):
        text = test_texts[idx]
        true_label = test_labels[idx]

        # Predict hatespeech/normal
        predicted_label_text, _, predicted_label = predict_hatespeech_and_offensiveness(model, text, tokenizer)

        if predicted_label != true_label:
            misclassified.append((text, true_label, predicted_label))

    # Print up to `num_examples` random misclassified examples
    print(f"Total Misclassified Examples Found: {len(misclassified)}")
    if misclassified:
        for i in range(min(num_examples, len(misclassified))):
            random_idx = random.randint(0, len(misclassified) - 1)
            random_text, true_label, predicted_label = misclassified[random_idx]

            # Map labels to readable format
            true_label_text = "hatespeech" if true_label == 0 else "normal"
            predicted_label_text = "hatespeech" if predicted_label == 0 else "normal"

            print(f"\nExample {i + 1}:")
            print(f"Random Misclassified Sentence: {random_text}")
            print(f"True Label: {true_label_text}")
            print(f"Predicted Label: {predicted_label_text}")
    else:
        print("No misclassified examples found!")

# Test the function with the model and test dataset
print_misclassified_examples(model, tokenizer, test_texts, test_labels, num_examples=10)