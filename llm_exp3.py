# -*- coding: utf-8 -*-
"""LLM_exp3.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/15t4YwfZWl778GiRHeU9MIjU3tD-m7QiK
"""

!pip install datasets

pip install bitsandbytes

import random
import re
import torch
from collections import Counter
from datasets import load_dataset
from transformers import (
    AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, pipeline
)
from sklearn.metrics import accuracy_score, classification_report
from tqdm import tqdm

import random
from collections import Counter
import re
from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig
import torch

label_map = {0: "hatespeech", 1: "normal", 2: "offensive"}

def majority_label(label_list):
    counts = Counter(label_list)
    if not counts:
        return "unknown"
    return label_map[counts.most_common(1)[0][0]]

def extract_label_json(text):
    match = re.search(r'"label"\s*:\s*"?(normal|offensive|hatespeech)"?', text.lower())
    return match.group(1) if match else "unknown"


def format_prompt_fewshot_json(test_text, fewshot_data):
    prompt = (
        'System: You are a content moderation engine. From the text classify it with only ONE label from the following 3 : normal, offensive, hatespeech '
        'Only return your output in this JSON format:\n{\n  "label": "<normal|offensive|hatespeech>",\n  "explanation": "<only one short sentence>"\n}\n\n'
    )
    for sample in fewshot_data:
        example_text = " ".join(sample["post_tokens"])
        label = majority_label(sample["annotators"]["label"])
        prompt += f'Example:\nUser: {example_text}\nAssistant:\n{{"label": "{label}"}}\n\n'
    prompt += f"Now classify this:\nUser: {test_text}\n"
    return prompt

from huggingface_hub import login

# Paste your Hugging Face token here
login(token="...")

from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, pipeline
import torch

def load_model(model_name="google/gemma-7b-it", max_new_tokens=64, use_4bit=True):
    is_cuda = torch.cuda.is_available()

    if is_cuda and use_4bit:
        quant_config = BitsAndBytesConfig(
            load_in_4bit=True,
            bnb_4bit_use_double_quant=True,
            bnb_4bit_compute_dtype=torch.float16,  # ‚úÖ required
            llm_int8_enable_fp32_cpu_offload=False  # ‚úÖ disable CPU offload if staying fully on GPU
        )
    else:
        quant_config = None  # fallback to full precision if CPU

    tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)
    model = AutoModelForCausalLM.from_pretrained(
        model_name,
        device_map="cuda" if (is_cuda and use_4bit) else "cpu",  # ‚úÖ force CUDA if available
        torch_dtype=torch.float16 if is_cuda else torch.float32,
        quantization_config=quant_config
    )

    pipe = pipeline(
        "text-generation",
        model=model,
        tokenizer=tokenizer,
        max_new_tokens=max_new_tokens,
        truncation=True,
        return_full_text=True
    )

    print("‚úÖ Model loaded on:", next(model.parameters()).device)
    return pipe

def run_fewshot_inference(pipe, dataset, fewshot_data, batch_size=2):
    from tqdm import tqdm

    prompts = [format_prompt_fewshot_json(" ".join(row["post_tokens"]), fewshot_data) for row in dataset]

    generated = []
    for i in tqdm(range(0, len(prompts), batch_size), desc="Running inference"):
        batch = prompts[i:i + batch_size]
        try:
            outputs = pipe(batch, do_sample=False)
            for result in outputs:
                if isinstance(result, list):
                    generated.extend(result)
                else:
                    generated.append(result)
        except Exception as e:
            print(f"‚ùå Error in batch {i}: {e}")

    return generated

def extract_golds_preds(dataset, generated):
    from collections import Counter

    def majority_label(label_list):
        counts = Counter(label_list)
        if not counts:
            return "unknown"
        return ["hatespeech", "normal", "offensive"][counts.most_common(1)[0][0]]

    def extract_label(text):
        # Match **Label:** line
        matches = re.findall(r"\*\*label:\*\*\s*(hatespeech|normal|offensive)", text.lower())
        if matches:
            return matches[-1]
        # Fallback to JSON
        fallback = re.findall(r'"label"\s*:\s*"?(hatespeech|normal|offensive)"?', text.lower())
        if fallback:
            return fallback[-1]
        return "unknown"

    golds = [majority_label(sample["annotators"]["label"]) for sample in dataset]
    preds = [extract_label(item.get("generated_text", "")) for item in generated]

    return golds, preds

def evaluate_predictions(golds, preds):
    print("‚úÖ Accuracy:", accuracy_score(golds, preds))
    print("\nüìä Classification Report:")
    print(classification_report(golds, preds, digits=3))

# Load dataset
from datasets import load_dataset
import random

# Load a larger portion first
full_validation = load_dataset("hatexplain", split="validation", trust_remote_code=True)

dataset = full_validation.select(range(950, 1050))  # upper bound is exclusive


# Sample 3 few-shot examples
fewshot_data = random.sample(list(dataset), 3)

from collections import Counter

label_map = {0: "hatespeech", 1: "normal", 2: "offensive"}

def majority_label(label_list):
    count = Counter(label_list)
    return label_map[count.most_common(1)[0][0]]

def count_majority_labels_first_x(dataset, x):
    labels = []
    for i in range(x):
        sample = dataset[int(i)]  # ‚úÖ use integer indexing, not slicing
        raw_labels = sample["annotators"]["label"]  # ‚úÖ this gives [1, 1, 2], etc.
        labels.append(majority_label(raw_labels))
    return Counter(labels)

# Example: count majority labels in first 50
label_counts = count_majority_labels_first_x(dataset, 100)
print(f"üìä Majority label distribution in first 50 samples:\n{label_counts}")

pipe=0

# Load model (set use_4bit=False for CPU)
pipe = load_model("google/gemma-7b-it", max_new_tokens=128, use_4bit=True)

generated=0

generated = run_fewshot_inference(pipe, dataset, fewshot_data, batch_size=4)

golds, preds = extract_golds_preds(dataset, generated)

evaluate_predictions(golds, preds)

for i, gen in enumerate(generated[:5]):
    print(f"\n--- Output {i+1} ---\n{gen['generated_text']}\n")

print(type(generated[0]))  # <class 'list'>
print(generated[0])

evaluate_predictions(golds, preds)

import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay

def plot_confusion_matrix(y_true, y_pred, labels=["hatespeech", "normal", "offensive"]):
    cm = confusion_matrix(y_true, y_pred, labels=labels)
    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=labels)
    fig, ax = plt.subplots(figsize=(6, 6))
    disp.plot(ax=ax, cmap='Blues', values_format='d')
    plt.title("Confusion Matrix")
    plt.show()

plot_confusion_matrix(golds, preds)

from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, ConfusionMatrixDisplay
import matplotlib.pyplot as plt

def log_metrics(golds, preds, labels=["normal", "offensive", "hatespeech"]):
    # Accuracy
    acc = accuracy_score(golds, preds)
    print(f"‚úÖ Accuracy: {acc:.3f}")

    # Macro-F1
    f1 = f1_score(golds, preds, average="macro")
    print(f"üìä Macro-F1: {f1:.3f}")

    # Confusion Matrix
    print("\nüîç Confusion Matrix:")
    cm = confusion_matrix(golds, preds, labels=labels)
    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=labels)
    fig, ax = plt.subplots(figsize=(6, 6))
    disp.plot(ax=ax, cmap="Blues", values_format="d")
    plt.title("Confusion Matrix")
    plt.show()

log_metrics(golds, preds)

def extract_golds_preds_any_match(dataset, generated):
    import re

    def extract_label(text):
        matches = re.findall(r"\*\*label:\*\*\s*(hatespeech|normal|offensive)", text.lower())
        if matches:
            return matches[-1]
        fallback = re.findall(r'"label"\s*:\s*"?(hatespeech|normal|offensive)"?', text.lower())
        if fallback:
            return fallback[-1]
        return "unknown"

    label_map = {0: "hatespeech", 1: "normal", 2: "offensive"}

    preds = [extract_label(item.get("generated_text", "")) for item in generated]

    # Create list of 1 if correct, 0 if wrong (custom match logic)
    gold_match_flags = []
    gold_expanded = []

    for i, sample in enumerate(dataset):
        annotator_labels = [label_map[x] for x in sample["annotators"]["label"]]
        pred = preds[i]

        if pred in annotator_labels:
            gold_match_flags.append(1)  # correct
        else:
            gold_match_flags.append(0)

        # For metrics later
        gold_expanded.append(pred)  # we use pred again just to keep same length

    return gold_match_flags, preds

correct_flags, preds = extract_golds_preds_any_match(dataset, generated)

accuracy = sum(correct_flags) / len(correct_flags)
print(f"‚úÖ Custom Accuracy (if any annotator agrees): {accuracy:.3f}")

import random

def print_random_example_by_true_label(dataset, generated, golds, preds, target_label="normal"):
    # Find all matching indices
    matching_indices = [i for i, label in enumerate(golds) if label == target_label]

    if not matching_indices:
        print(f"‚ùå No examples found with true label = '{target_label}'")
        return

    # Pick one randomly
    idx = random.choice(matching_indices)

    print(f"üéØ Random example with true label = '{target_label}'\n")
    print(f"üìå Sample index: {idx}")
    print(f"üü¢ True Label:    {golds[idx]}")
    print(f"üîÅ Predicted:     {preds[idx]}")
    print("üìÑ Text:\n", " ".join(dataset[idx]["post_tokens"]))
    print("\nüß† Model Output:\n", generated[idx]["generated_text"])
    print("\n" + "-" * 80 + "\n")

print_random_example_by_true_label(dataset, generated, golds, preds, target_label="normal")

pipe = load_model("mistralai/Mistral-7B-Instruct-v0.3", max_new_tokens=128, use_4bit=True)



generated = run_fewshot_inference(pipe, dataset, fewshot_data, batch_size=4)

golds, preds = extract_golds_preds(dataset, generated)

evaluate_predictions(golds, preds)

for i, gen in enumerate(generated[:5]):
    print(f"\n--- Output {i+1} ---\n{gen['generated_text']}\n")

print(type(generated[0]))  # <class 'list'>
print(generated[0])

evaluate_predictions(golds, preds)

import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay

def plot_confusion_matrix(y_true, y_pred, labels=["hatespeech", "normal", "offensive"]):
    cm = confusion_matrix(y_true, y_pred, labels=labels)
    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=labels)
    fig, ax = plt.subplots(figsize=(6, 6))
    disp.plot(ax=ax, cmap='Blues', values_format='d')
    plt.title("Confusion Matrix")
    plt.show()

plot_confusion_matrix(golds, preds)

from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, ConfusionMatrixDisplay
import matplotlib.pyplot as plt

def log_metrics(golds, preds, labels=["normal", "offensive", "hatespeech"]):
    # Accuracy
    acc = accuracy_score(golds, preds)
    print(f"‚úÖ Accuracy: {acc:.3f}")

    # Macro-F1
    f1 = f1_score(golds, preds, average="macro")
    print(f"üìä Macro-F1: {f1:.3f}")

    # Confusion Matrix
    print("\nüîç Confusion Matrix:")
    cm = confusion_matrix(golds, preds, labels=labels)
    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=labels)
    fig, ax = plt.subplots(figsize=(6, 6))
    disp.plot(ax=ax, cmap="Blues", values_format="d")
    plt.title("Confusion Matrix")
    plt.show()

log_metrics(golds, preds)

def extract_golds_preds_any_match(dataset, generated):
    import re

    def extract_label(text):
        matches = re.findall(r"\*\*label:\*\*\s*(hatespeech|normal|offensive)", text.lower())
        if matches:
            return matches[-1]
        fallback = re.findall(r'"label"\s*:\s*"?(hatespeech|normal|offensive)"?', text.lower())
        if fallback:
            return fallback[-1]
        return "unknown"

    label_map = {0: "hatespeech", 1: "normal", 2: "offensive"}

    preds = [extract_label(item.get("generated_text", "")) for item in generated]

    # Create list of 1 if correct, 0 if wrong (custom match logic)
    gold_match_flags = []
    gold_expanded = []

    for i, sample in enumerate(dataset):
        annotator_labels = [label_map[x] for x in sample["annotators"]["label"]]
        pred = preds[i]

        if pred in annotator_labels:
            gold_match_flags.append(1)  # correct
        else:
            gold_match_flags.append(0)

        # For metrics later
        gold_expanded.append(pred)  # we use pred again just to keep same length

    return gold_match_flags, preds

correct_flags, preds = extract_golds_preds_any_match(dataset, generated)

accuracy = sum(correct_flags) / len(correct_flags)
print(f"‚úÖ Custom Accuracy (if any annotator agrees): {accuracy:.3f}")

import random

def print_random_example_by_true_label(dataset, generated, golds, preds, target_label="normal"):
    # Find all matching indices
    matching_indices = [i for i, label in enumerate(golds) if label == target_label]

    if not matching_indices:
        print(f"‚ùå No examples found with true label = '{target_label}'")
        return

    # Pick one randomly
    idx = random.choice(matching_indices)

    print(f"üéØ Random example with true label = '{target_label}'\n")
    print(f"üìå Sample index: {idx}")
    print(f"üü¢ True Label:    {golds[idx]}")
    print(f"üîÅ Predicted:     {preds[idx]}")
    print("üìÑ Text:\n", " ".join(dataset[idx]["post_tokens"]))
    print("\nüß† Model Output:\n", generated[idx]["generated_text"])
    print("\n" + "-" * 80 + "\n")

print_random_example_by_true_label(dataset, generated, golds, preds, target_label="normal")

pipe = load_model("meta-llama/Meta-Llama-3-8B-Instruct", max_new_tokens=128, use_4bit=True)

generated = run_fewshot_inference(pipe, dataset, fewshot_data, batch_size=4)

golds, preds = extract_golds_preds(dataset, generated)

evaluate_predictions(golds, preds)

for i, gen in enumerate(generated[:5]):
    print(f"\n--- Output {i+1} ---\n{gen['generated_text']}\n")

print(type(generated[0]))  # <class 'list'>
print(generated[0])

evaluate_predictions(golds, preds)

import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay

def plot_confusion_matrix(y_true, y_pred, labels=["hatespeech", "normal", "offensive"]):
    cm = confusion_matrix(y_true, y_pred, labels=labels)
    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=labels)
    fig, ax = plt.subplots(figsize=(6, 6))
    disp.plot(ax=ax, cmap='Blues', values_format='d')
    plt.title("Confusion Matrix")
    plt.show()

plot_confusion_matrix(golds, preds)

from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, ConfusionMatrixDisplay
import matplotlib.pyplot as plt

def log_metrics(golds, preds, labels=["normal", "offensive", "hatespeech"]):
    # Accuracy
    acc = accuracy_score(golds, preds)
    print(f"‚úÖ Accuracy: {acc:.3f}")

    # Macro-F1
    f1 = f1_score(golds, preds, average="macro")
    print(f"üìä Macro-F1: {f1:.3f}")

    # Confusion Matrix
    print("\nüîç Confusion Matrix:")
    cm = confusion_matrix(golds, preds, labels=labels)
    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=labels)
    fig, ax = plt.subplots(figsize=(6, 6))
    disp.plot(ax=ax, cmap="Blues", values_format="d")
    plt.title("Confusion Matrix")
    plt.show()

log_metrics(golds, preds)

def extract_golds_preds_any_match(dataset, generated):
    import re

    def extract_label(text):
        matches = re.findall(r"\*\*label:\*\*\s*(hatespeech|normal|offensive)", text.lower())
        if matches:
            return matches[-1]
        fallback = re.findall(r'"label"\s*:\s*"?(hatespeech|normal|offensive)"?', text.lower())
        if fallback:
            return fallback[-1]
        return "unknown"

    label_map = {0: "hatespeech", 1: "normal", 2: "offensive"}

    preds = [extract_label(item.get("generated_text", "")) for item in generated]

    # Create list of 1 if correct, 0 if wrong (custom match logic)
    gold_match_flags = []
    gold_expanded = []

    for i, sample in enumerate(dataset):
        annotator_labels = [label_map[x] for x in sample["annotators"]["label"]]
        pred = preds[i]

        if pred in annotator_labels:
            gold_match_flags.append(1)  # correct
        else:
            gold_match_flags.append(0)

        # For metrics later
        gold_expanded.append(pred)  # we use pred again just to keep same length

    return gold_match_flags, preds

correct_flags, preds = extract_golds_preds_any_match(dataset, generated)

accuracy = sum(correct_flags) / len(correct_flags)
print(f"‚úÖ Custom Accuracy (if any annotator agrees): {accuracy:.3f}")

import random

def print_random_example_by_true_label(dataset, generated, golds, preds, target_label="normal"):
    # Find all matching indices
    matching_indices = [i for i, label in enumerate(golds) if label == target_label]

    if not matching_indices:
        print(f"‚ùå No examples found with true label = '{target_label}'")
        return

    # Pick one randomly
    idx = random.choice(matching_indices)

    print(f"üéØ Random example with true label = '{target_label}'\n")
    print(f"üìå Sample index: {idx}")
    print(f"üü¢ True Label:    {golds[idx]}")
    print(f"üîÅ Predicted:     {preds[idx]}")
    print("üìÑ Text:\n", " ".join(dataset[idx]["post_tokens"]))
    print("\nüß† Model Output:\n", generated[idx]["generated_text"])
    print("\n" + "-" * 80 + "\n")

print_random_example_by_true_label(dataset, generated, golds, preds, target_label="offensive")